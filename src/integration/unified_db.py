"""
í†µí•©DB êµ¬ì¶• ëª¨ë“ˆ

ëª¨ë“  ë§¤ë¬¼DB ì‹œíŠ¸ì˜ ë°ì´í„°ë¥¼ í†µí•©DBë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import pandas as pd
from typing import List, Dict, Any, Optional
from loguru import logger

from src.config.settings import Settings
from src.sheets.reader import SheetsReader
from src.sheets.writer import SheetsWriter


class UnifiedDBBuilder:
    """í†µí•©DB êµ¬ì¶• í´ë˜ìŠ¤"""
    
    def __init__(self, settings: Settings):
        """
        í†µí•©DB êµ¬ì¶• ì´ˆê¸°í™”
        
        Args:
            settings: ì‹œìŠ¤í…œ ì„¤ì • ê°ì²´
        """
        self.settings = settings
        self.reader = SheetsReader(settings)
        self.writer = SheetsWriter(settings)
        self.unified_sheet_name = 'í†µí•©DB'
        
        # ì‹œíŠ¸ íƒ€ì…ë³„ D_ID ì»¬ëŸ¼ëª… ë§¤í•‘
        self.d_id_column_mapping = {
            'ì•„íŒŒíŠ¸ë§¤ë¬¼': 'D_AD_ID',
            'ì£¼íƒíƒ€ìš´': 'D_H_ID',
            'ìƒê°€': 'D_S_ID',
            'ì›íˆ¬ë£¸': 'D_O_ID',
            'ê±´ë¬¼': 'D_B_ID',
            'ê³µì¥ì°½ê³ ': 'D_F_ID',
            'í† ì§€': 'D_L_ID'
        }
        
    def build_unified_db(self) -> bool:
        """
        í†µí•©DB êµ¬ì¶• (D_IDì™€ ì£¼ì†Œ ì¹¼ëŸ¼)
        
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("ğŸ”„ í†µí•©DB êµ¬ì¶• ì‹œì‘")
            
            # 1. ëª¨ë“  ì‹œíŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            all_sheets = self.reader.get_all_sheet_names()
            logger.info(f"ğŸ“‹ ì „ì²´ ì‹œíŠ¸ ìˆ˜: {len(all_sheets)}")
            
            # 2. ë§¤ë¬¼DB ì‹œíŠ¸ ì°¾ê¸° (ë§¤ë¬¼DBë¡œ ëë‚˜ëŠ” ì‹œíŠ¸ë“¤)
            property_sheets = self._find_property_sheets(all_sheets)
            logger.info(f"ğŸ  ë§¤ë¬¼DB ì‹œíŠ¸ ë°œê²¬: {len(property_sheets)} ê°œ")
            
            if not property_sheets:
                logger.warning("âš ï¸ ë§¤ë¬¼DB ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # 3. ê° ë§¤ë¬¼DB ì‹œíŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
            unified_data = []
            for sheet_name in property_sheets:
                logger.info(f"ğŸ“– ë°ì´í„° ìˆ˜ì§‘ ì¤‘: {sheet_name}")
                sheet_data = self._collect_sheet_data(sheet_name)
                if sheet_data:
                    unified_data.extend(sheet_data)
            
            if not unified_data:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            logger.info(f"âœ… ì´ {len(unified_data)} ê°œì˜ ë ˆì½”ë“œ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 4. í†µí•©DB ì‹œíŠ¸ì— ë°ì´í„° ì“°ê¸°
            success = self._write_to_unified_db(unified_data)
            
            if success:
                logger.info(f"âœ… í†µí•©DB êµ¬ì¶• ì™„ë£Œ: {self.unified_sheet_name}")
            else:
                logger.error("âŒ í†µí•©DB êµ¬ì¶• ì‹¤íŒ¨")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ í†µí•©DB êµ¬ì¶• ì˜¤ë¥˜: {e}")
            return False
    
    def _find_property_sheets(self, all_sheets: List[str]) -> List[str]:
        """
        ë§¤ë¬¼DB ì‹œíŠ¸ ì°¾ê¸°
        
        Args:
            all_sheets: ì „ì²´ ì‹œíŠ¸ ëª©ë¡
            
        Returns:
            List[str]: ë§¤ë¬¼DB ì‹œíŠ¸ ëª©ë¡
        """
        # ë§¤ë¬¼ ì‹œíŠ¸ ëª©ë¡ (ëª…ì‹œì  ì§€ì •)
        property_sheet_names = [
            'ì•„íŒŒíŠ¸ë§¤ë¬¼',
            'ì£¼íƒíƒ€ìš´',
            'ê±´ë¬¼',
            'ìƒê°€',
            'ì›íˆ¬ë£¸',
            'ê³µì¥ì°½ê³ ',
            'í† ì§€'
        ]
        
        property_sheets = []
        
        # ì œì™¸í•  ì‹œíŠ¸ ëª©ë¡
        exclude_sheets = [
            'í†µí•©DB',
            'ë“±ë¡ê²€ìƒ‰',
            'ê³ ê°DB',
            'ì•„íŒŒíŠ¸ë‹¨ì§€',
            'ê³ ì •ê°’',
            'ê³ ì •ê°’(ìˆ˜ì •X)',
            'ëŒ€ì‹œë³´ë“œ',
            'ë¶„ì–‘ê°€',
            'ì˜µì…˜',
            'íƒ€ì…',
            'ë°œì½”ë‹ˆ',
            'ë‹¨ì§€ì¼ì •'
        ]
        
        for sheet_name in all_sheets:
            # ì œì™¸ ëª©ë¡ì— ì—†ê³ , ë§¤ë¬¼ ì‹œíŠ¸ ëª©ë¡ì— ìˆê±°ë‚˜ 'DB'/'ë§¤ë¬¼'ì´ í¬í•¨ëœ ì‹œíŠ¸
            if sheet_name not in exclude_sheets:
                if sheet_name in property_sheet_names or 'DB' in sheet_name or 'ë§¤ë¬¼' in sheet_name:
                    property_sheets.append(sheet_name)
        
        return property_sheets
    
    def _get_d_id_column_name(self, sheet_name: str) -> Optional[str]:
        """
        ì‹œíŠ¸ ì´ë¦„ìœ¼ë¡œë¶€í„° D_ID ì»¬ëŸ¼ëª… ì°¾ê¸°
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            
        Returns:
            Optional[str]: D_ID ì»¬ëŸ¼ëª… (ì—†ìœ¼ë©´ None)
        """
        # ì‹œíŠ¸ ì´ë¦„ì—ì„œ ë§¤ë¬¼ íƒ€ì… ì°¾ê¸°
        for property_type, column_name in self.d_id_column_mapping.items():
            if property_type in sheet_name:
                return column_name
        
        # ë§¤í•‘ì— ì—†ìœ¼ë©´ Dì—´(4ë²ˆì§¸ ì—´)ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
        logger.warning(f"âš ï¸ {sheet_name}: ì‹œíŠ¸ íƒ€ì…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ Dì—´ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤")
        return None
    
    def _collect_sheet_data(self, sheet_name: str) -> List[Dict[str, Any]]:
        """
        ì‹œíŠ¸ì—ì„œ í†µí•©DB ë°ì´í„° ìˆ˜ì§‘
        
        í†µí•©DB êµ¬ì¡°:
        - Aì—´: ID
        - Bì—´: ê´€ë ¨íŒŒì¼
        - Cì—´: í´ë”ID
        - Dì—´: D_ID (ë””ìŠ¤í”Œë ˆì´ ì•„ì´ë””)
        - Eì—´: ì£¼ì†Œ
        - Fì—´: ë§¤ë¬¼ìœ í˜• (ì›ë³¸ ì‹œíŠ¸ëª…)
        
        ê±´ë¬¼ ì‹œíŠ¸ì˜ ê²½ìš°:
        - Mì—´(13ë²ˆì§¸ ì—´)ì˜ "í†µë§¤ë§¤" ì²´í¬ë°•ìŠ¤ê°€ ì²´í¬ëœ ê²ƒë§Œ ë§¤ë¬¼ë¡œ ê°„ì£¼
        
        Args:
            sheet_name: ì‹œíŠ¸ ì´ë¦„
            
        Returns:
            List[Dict[str, Any]]: ìˆ˜ì§‘ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì‹œíŠ¸ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸°
            df = self.reader.read_sheet_as_dataframe(sheet_name)
            
            if df.empty:
                logger.warning(f"âš ï¸ ë¹ˆ ì‹œíŠ¸: {sheet_name}")
                return []
            
            collected_data = []
            
            # ê³ ì • ì»¬ëŸ¼ ìœ„ì¹˜ (A=ID, B=ê´€ë ¨íŒŒì¼, C=í´ë”ID, D=D_ID, E=ì£¼ì†Œ)
            id_col_idx = 0      # Aì—´ (0-based)
            url_col_idx = 1     # Bì—´ (0-based)
            folder_id_col_idx = 2  # Cì—´ (0-based)
            d_id_col_idx = 3    # Dì—´ (0-based)
            address_col_idx = 4 # Eì—´ (0-based)
            
            # ê±´ë¬¼ ì‹œíŠ¸ì˜ ê²½ìš° Mì—´(13ë²ˆì§¸ ì—´, ì¸ë±ìŠ¤ 12)ì˜ í†µë§¤ë§¤ ì²´í¬ë°•ìŠ¤ í™•ì¸
            is_building_sheet = sheet_name == 'ê±´ë¬¼'
            í†µë§¤ë§¤_col_idx = 12  # Mì—´ì€ 13ë²ˆì§¸ ì—´ (0-basedë¡œ 12)
            
            # ê° í–‰ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            for idx, row in df.iterrows():
                # ê±´ë¬¼ ì‹œíŠ¸ì˜ ê²½ìš° í†µë§¤ë§¤ ì²´í¬ë°•ìŠ¤ í™•ì¸
                if is_building_sheet:
                    if len(df.columns) > í†µë§¤ë§¤_col_idx:
                        í†µë§¤ë§¤_value = row.iloc[í†µë§¤ë§¤_col_idx] if pd.notna(row.iloc[í†µë§¤ë§¤_col_idx]) else ''
                        # ì²´í¬ë°•ìŠ¤ê°€ ì²´í¬ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                        # ì²´í¬ë°•ìŠ¤ ê°’ì€ ë³´í†µ TRUE/FALSE, "TRUE"/"FALSE", ë˜ëŠ” ì²´í¬ í‘œì‹œ
                        if not self._is_checked(í†µë§¤ë§¤_value):
                            continue
                    else:
                        # Mì—´ì´ ì—†ìœ¼ë©´ ëª¨ë“  ê±´ë¬¼ í¬í•¨ (í•˜ìœ„ í˜¸í™˜ì„±)
                        logger.debug(f"âš ï¸ {sheet_name}: Mì—´(í†µë§¤ë§¤)ì´ ì—†ì–´ ëª¨ë“  ê±´ë¬¼ í¬í•¨")
                
                # Aì—´: ID
                id_value = ''
                if len(df.columns) > id_col_idx:
                    id_value = str(row.iloc[id_col_idx]) if pd.notna(row.iloc[id_col_idx]) else ''
                
                # Bì—´: ê´€ë ¨íŒŒì¼
                url_value = ''
                if len(df.columns) > url_col_idx:
                    url_value = str(row.iloc[url_col_idx]) if pd.notna(row.iloc[url_col_idx]) else ''
                
                # Cì—´: í´ë”ID
                folder_id_value = ''
                if len(df.columns) > folder_id_col_idx:
                    folder_id_value = str(row.iloc[folder_id_col_idx]) if pd.notna(row.iloc[folder_id_col_idx]) else ''
                
                # Dì—´: D_ID (ë””ìŠ¤í”Œë ˆì´ ì•„ì´ë””)
                d_id = ''
                if len(df.columns) > d_id_col_idx:
                    d_id = str(row.iloc[d_id_col_idx]) if pd.notna(row.iloc[d_id_col_idx]) else ''
                
                # Eì—´: ì£¼ì†Œ
                address = ''
                if len(df.columns) > address_col_idx:
                    address = str(row.iloc[address_col_idx]) if pd.notna(row.iloc[address_col_idx]) else ''
                
                # ë¹ˆ D_IDëŠ” ê±´ë„ˆë›°ê¸°
                if not d_id or d_id.strip() == '':
                    continue
                
                collected_data.append({
                    'ID': id_value.strip(),
                    'ê´€ë ¨íŒŒì¼': url_value.strip(),
                    'í´ë”ID': folder_id_value.strip(),
                    'D_ID': d_id.strip(),
                    'ì£¼ì†Œ': address.strip(),
                    'ë§¤ë¬¼ìœ í˜•': sheet_name
                })
            
            logger.info(f"âœ… {sheet_name}: {len(collected_data)} ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
            return collected_data
            
        except Exception as e:
            logger.error(f"âŒ ì‹œíŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({sheet_name}): {e}")
            return []
    
    def _is_checked(self, value: Any) -> bool:
        """
        ì²´í¬ë°•ìŠ¤ ê°’ì´ ì²´í¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        
        Args:
            value: ì²´í¬ë°•ìŠ¤ ê°’ (TRUE/FALSE, "TRUE"/"FALSE", ì²´í¬ í‘œì‹œ ë“±)
            
        Returns:
            bool: ì²´í¬ë˜ì—ˆìœ¼ë©´ True
        """
        if pd.isna(value):
            return False
        
        value_str = str(value).strip().upper()
        
        # TRUE, "TRUE", ì²´í¬ í‘œì‹œ ë“±
        checked_values = ['TRUE', '1', 'YES', 'Y', 'âœ“', 'âœ”', 'CHECKED', 'ì²´í¬']
        
        return value_str in checked_values
    
    def _construct_address_from_row(self, row: pd.Series, columns: List[str]) -> str:
        """
        í–‰ ë°ì´í„°ì—ì„œ ì£¼ì†Œ êµ¬ì„± ì‹œë„
        í˜•ì‹: "ì‹œêµ°êµ¬ ë™ìë©´ í†µë°˜ë¦¬ ì§€ë²ˆ"
        ì˜ˆ: "ì¶©ì²­ë‚¨ë„ ì•„ì‚°ì‹œ ë°°ë°©ì ê³µìˆ˜ë¦¬ 1730"
        
        Args:
            row: í–‰ ë°ì´í„°
            columns: ì»¬ëŸ¼ ëª©ë¡
            
        Returns:
            str: êµ¬ì„±ëœ ì£¼ì†Œ
        """
        address_parts = []
        
        # ì‹œêµ°êµ¬ ì°¾ê¸°
        ì‹œêµ°êµ¬ = ''
        for col in ['ì‹œêµ°êµ¬', 'ì‹œë„', 'ì‹œ']:
            if col in columns and pd.notna(row[col]):
                ì‹œêµ°êµ¬ = str(row[col]).strip()
                if ì‹œêµ°êµ¬:
                    break
        
        # ë™ìë©´ ì°¾ê¸°
        ë™ìë©´ = ''
        for col in ['ë™ìë©´', 'ìë©´ë™', 'ë™']:
            if col in columns and pd.notna(row[col]):
                ë™ìë©´ = str(row[col]).strip()
                if ë™ìë©´:
                    break
        
        # í†µë°˜ë¦¬ ì°¾ê¸° (ì„ íƒì )
        í†µë°˜ë¦¬ = ''
        for col in ['í†µë°˜ë¦¬', 'ë¦¬', 'í†µ']:
            if col in columns and pd.notna(row[col]):
                í†µë°˜ë¦¬ = str(row[col]).strip()
                if í†µë°˜ë¦¬:
                    break
        
        # ì§€ë²ˆ ì°¾ê¸°
        ì§€ë²ˆ = ''
        for col in ['ì§€ë²ˆ', 'ë²ˆì§€', 'ë²ˆì§€ìˆ˜']:
            if col in columns and pd.notna(row[col]):
                ì§€ë²ˆ = str(row[col]).strip()
                if ì§€ë²ˆ:
                    break
        
        # ì£¼ì†Œ êµ¬ì„±: ì‹œêµ°êµ¬ ë™ìë©´ í†µë°˜ë¦¬ ì§€ë²ˆ
        if ì‹œêµ°êµ¬:
            address_parts.append(ì‹œêµ°êµ¬)
        if ë™ìë©´:
            address_parts.append(ë™ìë©´)
        if í†µë°˜ë¦¬:
            address_parts.append(í†µë°˜ë¦¬)
        if ì§€ë²ˆ:
            address_parts.append(ì§€ë²ˆ)
        
        return ' '.join(address_parts) if address_parts else ''
    
    def _write_to_unified_db(self, data: List[Dict[str, Any]]) -> bool:
        """
        í†µí•©DB ì‹œíŠ¸ì— ë°ì´í„° ì“°ê¸°
        
        í†µí•©DB êµ¬ì¡°:
        - Aì—´: ID (D_ID ê°’ ì‚¬ìš©)
        - Bì—´: ê´€ë ¨íŒŒì¼ (ë¹ˆ ê°’, ë‚˜ì¤‘ì— í´ë” URLë¡œ ì±„ì›Œì§ˆ ìˆ˜ ìˆìŒ)
        - Cì—´: í´ë”ID (ë¹ˆ ê°’, ë‚˜ì¤‘ì— í´ë” IDë¡œ ì±„ì›Œì§ˆ ìˆ˜ ìˆìŒ)
        - Dì—´: D_ID
        - Eì—´: ì£¼ì†Œ
        - Fì—´: ì¶œì²˜ì‹œíŠ¸
        
        Args:
            data: ì‘ì„±í•  ë°ì´í„°
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # DataFrame ìƒì„±
            df = pd.DataFrame(data)
            
            # ì¤‘ë³µ ì œê±° (D_ID ê¸°ì¤€)
            initial_count = len(df)
            df = df.drop_duplicates(subset=['D_ID'], keep='first')
            if len(df) < initial_count:
                logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {initial_count} â†’ {len(df)} ê°œ")
            
            # í†µí•©DB êµ¬ì¡°ì— ë§ê²Œ ì»¬ëŸ¼ ì¬êµ¬ì„±
            # ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°ì— ID, ê´€ë ¨íŒŒì¼, í´ë”IDê°€ í¬í•¨ë˜ì–´ ìˆìŒ
            # ì»¬ëŸ¼ ìˆœì„œ: ID, ê´€ë ¨íŒŒì¼, í´ë”ID, D_ID, ì£¼ì†Œ, ë§¤ë¬¼ìœ í˜•
            column_order = ['ID', 'ê´€ë ¨íŒŒì¼', 'í´ë”ID', 'D_ID', 'ì£¼ì†Œ', 'ë§¤ë¬¼ìœ í˜•']
            existing_columns = [col for col in column_order if col in df.columns]
            df = df[existing_columns]
            
            # í†µí•©DB ì‹œíŠ¸ì— ì“°ê¸°
            success = self.writer.update_sheet_with_dataframe(
                sheet_name=self.unified_sheet_name,
                dataframe=df,
                clear_existing=True
            )
            
            if success:
                logger.info(f"âœ… í†µí•©DB ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(df)} í–‰")
                logger.info(f"ğŸ“‹ ì»¬ëŸ¼ êµ¬ì¡°: A=ID, B=ê´€ë ¨íŒŒì¼, C=í´ë”ID, D=D_ID, E=ì£¼ì†Œ, F=ë§¤ë¬¼ìœ í˜•")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ í†µí•©DB ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def update_unified_db(self) -> bool:
        """
        í†µí•©DB ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•˜ë©´ì„œ ì¶”ê°€/ìˆ˜ì •)
        
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("ğŸ”„ í†µí•©DB ì—…ë°ì´íŠ¸ ì‹œì‘")
            
            # ê¸°ì¡´ í†µí•©DB ì½ê¸°
            existing_df = self.reader.read_sheet_as_dataframe(self.unified_sheet_name)
            
            # ëª¨ë“  ë§¤ë¬¼DB ì‹œíŠ¸ì—ì„œ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘
            all_sheets = self.reader.get_all_sheet_names()
            property_sheets = self._find_property_sheets(all_sheets)
            
            unified_data = []
            for sheet_name in property_sheets:
                sheet_data = self._collect_sheet_data(sheet_name)
                if sheet_data:
                    unified_data.extend(sheet_data)
            
            if not unified_data:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ìƒˆ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            new_df = pd.DataFrame(unified_data)
            
            # í†µí•©DB êµ¬ì¡°ì— ë§ê²Œ ì»¬ëŸ¼ ì •ë ¬
            column_order = ['ID', 'ê´€ë ¨íŒŒì¼', 'í´ë”ID', 'D_ID', 'ì£¼ì†Œ', 'ë§¤ë¬¼ìœ í˜•']
            existing_columns = [col for col in column_order if col in new_df.columns]
            new_df = new_df[existing_columns]
            
            # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (D_ID ê¸°ì¤€)
            if not existing_df.empty and 'D_ID' in existing_df.columns:
                # ê¸°ì¡´ ë°ì´í„°ì˜ ID, ê´€ë ¨íŒŒì¼, í´ë”IDëŠ” ìœ ì§€
                # ìƒˆ ë°ì´í„°ì™€ ë³‘í•© (D_ID ê¸°ì¤€)
                merged_df = pd.concat([existing_df, new_df], ignore_index=True)
                merged_df = merged_df.drop_duplicates(subset=['D_ID'], keep='last')
                
                # ë³‘í•© í›„ì—ë„ êµ¬ì¡° ìœ ì§€ (ID, ê´€ë ¨íŒŒì¼, í´ë”IDê°€ ì—†ìœ¼ë©´ ì¶”ê°€)
                if 'ID' not in merged_df.columns:
                    merged_df['ID'] = merged_df['D_ID']
                if 'ê´€ë ¨íŒŒì¼' not in merged_df.columns:
                    merged_df['ê´€ë ¨íŒŒì¼'] = ''
                if 'í´ë”ID' not in merged_df.columns:
                    merged_df['í´ë”ID'] = ''
                
                # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
                column_order = ['ID', 'ê´€ë ¨íŒŒì¼', 'í´ë”ID', 'D_ID', 'ì£¼ì†Œ', 'ë§¤ë¬¼ìœ í˜•']
                existing_columns = [col for col in column_order if col in merged_df.columns]
                other_columns = [col for col in merged_df.columns if col not in column_order]
                merged_df = merged_df[existing_columns + other_columns]
            else:
                merged_df = new_df
            
            # í†µí•©DB ì‹œíŠ¸ì— ì“°ê¸°
            success = self.writer.update_sheet_with_dataframe(
                sheet_name=self.unified_sheet_name,
                dataframe=merged_df,
                clear_existing=True
            )
            
            if success:
                logger.info(f"âœ… í†µí•©DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(merged_df)} í–‰")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ í†µí•©DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ì„¤ì • ë¡œë“œ
        settings = Settings()
        
        # í†µí•©DB êµ¬ì¶•
        builder = UnifiedDBBuilder(settings)
        success = builder.build_unified_db()
        
        if success:
            print("âœ… í†µí•©DB êµ¬ì¶• ì™„ë£Œ!")
        else:
            print("âŒ í†µí•©DB êµ¬ì¶• ì‹¤íŒ¨")
            return 1
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return 1


if __name__ == "__main__":
    exit(main())

